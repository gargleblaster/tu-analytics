{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jupyter-dash\n",
        "!pip install dash_daq\n",
        "!pip install dash-uploader\n",
        "#!pip install dash-uploader --pre"
      ],
      "metadata": {
        "id": "zT7kRAYUPyIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed29f13-3c51-412a-d237-9d9258f19afe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.8 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 357 kB 64.4 MB/s \n",
            "\u001b[?25h  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dash_daq\n",
            "  Downloading dash_daq-0.5.0.tar.gz (642 kB)\n",
            "\u001b[K     |████████████████████████████████| 642 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dash>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from dash_daq) (2.4.1)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (1.1.4)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (5.0.0)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (5.5.0)\n",
            "Requirement already satisfied: flask-compress in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (1.12)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.6.1->dash_daq) (2.0.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.6.1->dash_daq) (1.1.0)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.6.1->dash_daq) (2.11.3)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.6.1->dash_daq) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.6.1->dash_daq) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=1.6.1->dash_daq) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash>=1.6.1->dash_daq) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash>=1.6.1->dash_daq) (8.0.1)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.7/dist-packages (from flask-compress->dash>=1.6.1->dash_daq) (1.0.9)\n",
            "Building wheels for collected packages: dash-daq\n",
            "  Building wheel for dash-daq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-daq: filename=dash_daq-0.5.0-py3-none-any.whl size=669714 sha256=048f15f8074b17e4876a107292b9432018f596f215317d860dfb3e980ea3af08\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/54/53/a8d448db5592874db4313240571ca2c069e55f6a6b29bf5847\n",
            "Successfully built dash-daq\n",
            "Installing collected packages: dash-daq\n",
            "Successfully installed dash-daq-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dash-uploader\n",
            "  Downloading dash_uploader-0.6.0.tar.gz (491 kB)\n",
            "\u001b[K     |████████████████████████████████| 491 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dash>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dash-uploader) (2.4.1)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (1.1.4)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (5.0.0)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (5.5.0)\n",
            "Requirement already satisfied: flask-compress in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (1.12)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash>=1.1.0->dash-uploader) (2.0.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.1.0->dash-uploader) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.1.0->dash-uploader) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.1.0->dash-uploader) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.0.4->dash>=1.1.0->dash-uploader) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=1.1.0->dash-uploader) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash>=1.1.0->dash-uploader) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash>=1.1.0->dash-uploader) (8.0.1)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.7/dist-packages (from flask-compress->dash>=1.1.0->dash-uploader) (1.0.9)\n",
            "Building wheels for collected packages: dash-uploader\n",
            "  Building wheel for dash-uploader (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-uploader: filename=dash_uploader-0.6.0-py3-none-any.whl size=69214 sha256=547172d0a1831c3a63d8d79296d7080b02029642df757e0f4a1eb1335fc7c071\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/4d/1c/941d4c28aa60af88391d34dd3f33d03b03372b33551f956b33\n",
            "Successfully built dash-uploader\n",
            "Installing collected packages: dash-uploader\n",
            "Successfully installed dash-uploader-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "#!cp '/content/gdrive/MyDrive/lim_cprd.csv' '/content/'"
      ],
      "metadata": {
        "id": "QQkFEF4GLetn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "1pTkWY7Th4sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import pytz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from string import digits\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import datetime\n",
        "import os\n",
        "from pprint import pprint\n",
        "from IPython.core.display import display, HTML\n",
        "from IPython.display import HTML\n",
        "import plotly.express as px\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "import functools\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import random\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, mode\n",
        "from IPython.display import Javascript\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual, AppLayout, Button, Layout, GridspecLayout, HBox, VBox, Box, Label\n",
        "from IPython.display import clear_output\n",
        "import plotly.graph_objects as go"
      ],
      "metadata": {
        "id": "7B-XMcyEO3nx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from scipy.stats import linregress\n",
        "from jupyter_dash import JupyterDash\n",
        "#import dash_core_components as dcc\n",
        "from dash import Dash, dcc, html, Input, Output, State, callback_context, dcc\n",
        "from datetime import date\n",
        "import dash_daq as daq\n",
        "#import dash_core_components as dcc\n"
      ],
      "metadata": {
        "id": "UMBUTy73PzpZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dash_uploader as du"
      ],
      "metadata": {
        "id": "TIRQ5EYaDA2t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import datetime\n",
        "import io\n",
        "\n",
        "from dash.dependencies import Input, Output, State\n",
        "from dash import dcc, html, dash_table\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "SntS8QHo4qLm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Globals"
      ],
      "metadata": {
        "id": "VbegnhXdZEZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "line_x_axis_cat = ['reduced_symbols','tradePlanTemplate','full_symbols','setup_type','setupFilters_type','entries_type','exits_types','strategy','strategydescription', 'costbasis', 'quantity']\n",
        "line_x_axis = ['closedate','Datewise','reduced_symbols','realizedpnl',' percent_pnl', 'costbasis', 'quantity','tradePlanTemplate','costbasis','quantity','full_symbols','time-in-trade','trade_rate','setup_type','setupFilters_type','entries_type','exits_types','strategy','strategydescription','time_bracket']\n",
        "line_y_axis = ['realizedpnl', ' percent_pnl', 'time-in-trade', 'trade_rate', 'Count', 'costbasis', 'quantity']\n",
        "line_colors = ['reduced_symbols','tradePlanTemplate','full_symbols','setup_type','setupFilters_type','entries_type','exits_types','strategy','strategydescription']\n",
        "axis_options = ['realizedpnl',' percent_pnl','costbasis','quantity','closedate','strategy','strategydescription','tradePlanTemplate','Day of Week','reduced_symbols','full_symbols','time-in-trade','trade_rate','W/L','Long/Short Trades','Datewise','M','time_bracket','setup_type','setupFilters_type','entries_type','exits_types']\n",
        "ctgr_options = ['W/L','reduced_symbols','full_symbols','Long/Short Trades',' percent_pnl','trade_rate','strategy','strategydescription','tradePlanTemplate','time-in-trade','realizedpnl','Day of Week','M', 'costbasis', 'quantity']\n",
        "symb_options = ['W/L','reduced_symbols','full_symbols','Long/Short Trades','strategy','strategydescription','tradePlanTemplate']\n",
        "size_options = ['Count' ,'trade_rate','time-in-trade','realizedpnl',' percent_pnl', 'costbasis', 'quantity']\n",
        "fcet_options = ['W/L', 'Long/Short Trades']\n",
        "hver_options = ['strategy','tradePlanTemplate','lifecycle_det','setup_type','setup_det','setupFilters_type','setupFilters_det','options_det','entries_type','entries_det','exits_types','exits_dict','strategydescription','realizedpnl','time-in-trade','trade_rate','full_symbols','reduced_symbols','Day of Week', 'costbasis', 'quantity']\n",
        "\n",
        "result_cats = ['realizedpnl', ' percent_pnl', 'time-in-trade', 'trade_rate', 'costbasis', 'quantity']\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [15, 9]\n",
        "\n",
        "global_json_var = [] # to keep a tab of columns\n",
        "\n",
        "keyval_key = None"
      ],
      "metadata": {
        "id": "vT6pCjHnPMVi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lambda Functions"
      ],
      "metadata": {
        "id": "ZQwKKdm-ZH6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_symbols = lambda x: re.search(r\"[a-z]*\", x, re.IGNORECASE).group()\n",
        "cgr = lambda x: \"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
        "cnv = lambda x: (x-x%1)*100+(x%1)*60\n",
        "\n",
        "get_json_dict = lambda x: json.loads(x)\n",
        "get_json_keys = lambda x: [k for k in get_json_dict(x).keys()]\n",
        "get_json_keysLen = lambda x: len([k for k in get_json_dict(x).keys()])\n",
        "get_json_val1 = lambda x: np.nan if get_json_key1(x) is np.nan else get_json_dict(x)[get_json_key1(x)]\n",
        "\n",
        "converter = lambda x: x.dt.tz_convert(eastern)\n",
        "rate_calc = lambda x: np.round(x['realizedpnl']/x['time-in-trade'],4)\n",
        "win_chk = lambda x: 'win' if x>0 else 'loss'\n",
        "LScnv = lambda x: 'short' if x=='sell_short' else 'long'\n"
      ],
      "metadata": {
        "id": "0d-qosvTZKem"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "M0AR6nQBZK5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_key1(x):\n",
        "  try:\n",
        "    return get_json_keys(x)[0]\n",
        "  except:\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def clean_options(inp):\n",
        "  try:\n",
        "    stk_opts = []\n",
        "    for ckey in inp['optionChoices']['choices'].keys():\n",
        "      choices = inp['optionChoices']['choices'][ckey]\n",
        "      stk_opts += [clean_symbols(x) for x in choices]\n",
        "    stk_opts = list(dict.fromkeys(stk_opts))\n",
        "    inp['optionChoices'] = stk_opts\n",
        "    return inp\n",
        "  except:\n",
        "    try:\n",
        "      print(list(inp['optionChoices']['choices'].keys())[0])\n",
        "    except:\n",
        "      pass\n",
        "    return inp\n",
        "\n",
        "def time_bins(inp, bins=1440):\n",
        "\n",
        "  time_slots = cnv(np.arange(24/bins, 24+24/bins, 24/bins)).astype(int).tolist()\n",
        "  #lbls = cnv(np.arange(0, 24, 24/bins)).astype(int).astype(str).tolist()\n",
        "  lbls = np.arange(0, 24, 24/bins).tolist()\n",
        "\n",
        "  for i in range(len(time_slots)):\n",
        "    if inp<time_slots[i]:\n",
        "      return lbls[i]\n",
        "  return np.nan\n",
        "\n",
        "def time_bins_lim(inp, bins=96):\n",
        "\n",
        "  time_slots = cnv(np.arange(24/bins, 24+24/bins, 24/bins)).astype(int).tolist()\n",
        "  #lbls = cnv(np.arange(0, 24, 24/bins)).astype(int).astype(str).tolist()\n",
        "  lbls = np.arange(0, 24, 24/bins).tolist()\n",
        "\n",
        "  for i in range(len(time_slots)):\n",
        "    if inp<time_slots[i]:\n",
        "      return lbls[i]\n",
        "  return np.nan\n",
        "\n",
        "def process_stock_list(df, limit_list):\n",
        "  selected_stocks = []\n",
        "  if limit_list is () :\n",
        "    limit_list = ['All']\n",
        "  for selection in limit_list:\n",
        "    if selection == 'None':\n",
        "      pass\n",
        "    elif selection == 'All':\n",
        "      selected_stocks += df.reduced_symbols.unique().tolist()\n",
        "    elif selection == 'Quick Rising':\n",
        "      selected_stocks += df.groupby('reduced_symbols')['realizedpnl'].max().sort_values().reset_index()['reduced_symbols'].iloc[-4:].tolist()\n",
        "    elif selection == 'Worst':\n",
        "      selected_stocks += df.groupby('reduced_symbols')['realizedpnl'].min().sort_values().reset_index()['reduced_symbols'].iloc[:2].tolist()\n",
        "    elif selection == 'Safest':\n",
        "      selected_stocks += df.groupby('reduced_symbols')['realizedpnl'].min().sort_values().reset_index()['reduced_symbols'].iloc[-4:].tolist()\n",
        "    elif selection == 'Most Traded':\n",
        "      selected_stocks += df['reduced_symbols'].value_counts().reset_index()['index'].iloc[0:6].tolist()\n",
        "    elif selection == 'Most Impactful':\n",
        "      _tmp = df.groupby('reduced_symbols')['realizedpnl'].sum().sort_values().reset_index()\n",
        "      selected_stocks += np.concatenate([ _tmp['reduced_symbols'].iloc[:2].to_numpy(), _tmp['reduced_symbols'].iloc[-2:].to_numpy() ]).tolist()\n",
        "    else:\n",
        "      selected_stocks += [selection]\n",
        "  selected_stocks = list(dict.fromkeys(selected_stocks))\n",
        "  return selected_stocks\n",
        "\n",
        "def filter_nans(inp):\n",
        "  for col in inp.columns:\n",
        "    inp = inp[inp[col]!=np.nan]\n",
        "    inp = inp[inp[col]!=None]\n",
        "    inp = inp.dropna()\n",
        "  return inp\n",
        "\n",
        "def filter_outliers(inp, columns_to_check = ['realizedpnl']):\n",
        "  for col in columns_to_check:\n",
        "    inp = inp[(np.abs(stats.zscore(inp[col])) < 3)]\n",
        "  return inp\n",
        "\n",
        "\n",
        "\n",
        "# Avoids scroll-in-the-scroll in the entire Notebook\n",
        "def resize_colab_cell():\n",
        "  #print(\"Enter Resize\")\n",
        "  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'))\n",
        "  #print(\"Resized\")\n",
        "#get_ipython().events.register('pre_run_cell', resize_colab_cell)\n",
        "\n",
        "\n",
        "def get_var():\n",
        "  global global_json_var\n",
        "  return global_json_var\n",
        "def set_var():\n",
        "  global global_json_var\n",
        "  global_json_var = []\n",
        "\n",
        "def extract_roots(inp):\n",
        "  try:\n",
        "    inp = json.loads(inp)\n",
        "  except:\n",
        "    print(inp)\n",
        "    return ''\n",
        "  roots = ''\n",
        "  for key in inp.keys():\n",
        "    roots += str(key)\n",
        "    roots += ', '\n",
        "  return roots[:-2]\n",
        "\n",
        "def extract_leaves_list(inp, root=None):\n",
        "  outs = []\n",
        "  if isinstance(inp, dict):\n",
        "    for key in inp.keys():\n",
        "      if isinstance(inp[key], dict):\n",
        "        if root is None:\n",
        "          outs += extract_leaves_list(inp[key], key)\n",
        "        else:\n",
        "          outs += extract_leaves_list(inp[key], root)\n",
        "      else:\n",
        "        if (root is None) or (root is ''):\n",
        "          outs += [ str(key)+' == '+str(inp[key]) ]\n",
        "        else:\n",
        "          try:\n",
        "            if key[4]=='-':\n",
        "              outs += [ str(root)+' :: '+str(root)+' == '+str(inp[key]) ]\n",
        "            else:\n",
        "              outs += [ str(root)+' :: '+str(key)+' == '+str(inp[key]) ]\n",
        "          except:\n",
        "            outs += [ str(root)+' :: '+str(key)+' == '+str(inp[key]) ]\n",
        "    return outs\n",
        "  else:\n",
        "    return []\n",
        "\n",
        "def extract_leaves(inp, root=None, filters=[]):\n",
        "  global global_json_var\n",
        "  try:\n",
        "    inp = json.loads(inp)\n",
        "  except:\n",
        "    print(inp)\n",
        "    return {}\n",
        "  inp = extract_leaves_list(inp, root)\n",
        "  for filt in filters:\n",
        "    _c = inp.copy()\n",
        "    for item in _c:\n",
        "      if filt in item:\n",
        "        inp.remove(item)\n",
        "  out_dict = {}\n",
        "  for entry in inp:\n",
        "    entry = entry.split(' == ')\n",
        "    if entry[0] not in global_json_var:\n",
        "      global_json_var += [entry[0]]\n",
        "    try:\n",
        "      out_dict[entry[0]] = float(entry[1])\n",
        "    except:\n",
        "      out_dict[entry[0]] = str(entry[1])\n",
        "  #print(out_dict)\n",
        "  return out_dict\n",
        "  #return inp\n",
        "  #return '\\n'.join(inp)\n",
        "\n",
        "\n",
        "def set_keyval(key):\n",
        "  global keyval_key\n",
        "  keyval_key = key\n",
        "def keyval_finderr(inp):\n",
        "  try:\n",
        "    return inp[keyval_key]\n",
        "  except:\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def expand_into_columns(df_full, inp_col, col_list):\n",
        "  for col in col_list:\n",
        "    #keyval_finder = lambda x: (x[col] if col in x.keys() else np.nan) if isinstance(x, dict) else np.nan\n",
        "    keyval_finder = lambda x: (x[col] if col in x.keys() else -1) if isinstance(x, dict) else -1\n",
        "    df_full[col] = df_full[inp_col].apply(keyval_finder)\n",
        "  return df_full.copy()"
      ],
      "metadata": {
        "id": "X6Fb2rBXZNbF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Functions"
      ],
      "metadata": {
        "id": "IA7pBNn4ZOEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def time_dist_plots(wdf):\n",
        "  stkslcts = wdf.reduced_symbols.unique()\n",
        "  ax = wdf[wdf['reduced_symbols']==stkslcts[0]].sort_values('time_bracket').plot.scatter(x=\"time_bracket\", y=\"realizedpnl\",rot=45, color = cgr(None), label=stkslcts[0], alpha=0.7)\n",
        "  plt.locator_params(axis='x', nbins=6)\n",
        "  for i in range(1,len(stkslcts)):\n",
        "    wdf[wdf['reduced_symbols']==stkslcts[i]].sort_values('time_bracket').plot.scatter(x=\"time_bracket\", y=\"realizedpnl\",rot=45, color = cgr(None), label=stkslcts[i], ax=ax, alpha=0.7)\n",
        "  #plt.grid(b=True, which='major')\n",
        "  #ax.set_yscale('symlog', linthreshy=600)\n",
        "  if np.abs(wdf['realizedpnl'].max())>400:\n",
        "    plt.ylim(-300,300)\n",
        "  plt.title('Trades Distribution w.r.t. Time Brackets (UTC)')\n",
        "\n",
        "  plt.show()\n",
        "  \"\"\"\n",
        "  ax = wdf[['time_bracket']].value_counts().sort_index().plot.bar(stacked=True, rot=45, color = cgr(None), label=stkslcts[0], alpha=1)\n",
        "  plt.grid(b=True, which='major')\n",
        "  plt.show()\n",
        "  \"\"\"\n",
        "  wdf[['time_bracket_lim','reduced_symbols','Count']].groupby(['reduced_symbols','time_bracket_lim']).sum().reset_index().pivot(index='time_bracket_lim', columns='reduced_symbols', values='Count').replace({np.nan:0}).plot.bar(stacked=True, rot=45)\n",
        "  plt.grid(b=True, which='major')\n",
        "  plt.title(\"Number of trades made during each time bracket\")\n",
        "  #plt.locator_params(axis='x', nbins=6)\n",
        "  plt.show()\n",
        "\n",
        "def cumulative_plots(DF):\n",
        "  wdf = DF.copy()\n",
        "  stkslcts = wdf.reduced_symbols.unique()\n",
        "  for stock in stkslcts:\n",
        "    cum_stocks = lambda x: x['realizedpnl'] if x['reduced_symbols']==stock else 0\n",
        "    r_stocks = lambda x: x['trade_rate'] if x['reduced_symbols']==stock else 0\n",
        "    pos_vals = lambda x: x['c-'+stock] if x['c-'+stock]>0 else 0\n",
        "    neg_vals = lambda x: x['c-'+stock] if x['c-'+stock]<0 else 0\n",
        "    wdf[stock] = wdf[['realizedpnl', 'reduced_symbols']].apply(cum_stocks, axis=1)\n",
        "    wdf['c-'+stock] = wdf[stock].cumsum()\n",
        "    wdf['r-'+stock] = wdf[['trade_rate', 'reduced_symbols']].apply(r_stocks, axis=1)\n",
        "    wdf['pos-'+stock] = wdf[['c-'+stock, 'reduced_symbols']].apply(pos_vals, axis=1)\n",
        "    wdf['neg-'+stock] = wdf[['c-'+stock, 'reduced_symbols']].apply(neg_vals, axis=1)\n",
        "  _tmp = ['pos-'+x for x in stkslcts]\n",
        "  wdf['pos_acm'] = wdf[_tmp].sum(axis=1)\n",
        "  _tmp = ['neg-'+x for x in stkslcts]\n",
        "  wdf['neg_acm'] = wdf[_tmp].sum(axis=1)\n",
        "  _tmp = ['neg-'+x for x in stkslcts]\n",
        "  wdf.set_index('closedate', inplace=True)\n",
        "\n",
        "  _tmp = ['pnl_accumulated']+['c-'+x for x in stkslcts]\n",
        "  _rtmp = ['r-'+x for x in stkslcts]\n",
        "  ax = wdf[_tmp].plot(kind=\"line\", rot=45)\n",
        "  plt.grid(b=True, which='major')\n",
        "  ax.set_yscale('symlog', linthreshy=600)\n",
        "  plt.title('Cumulative PNL of Trades (Total and symbol-wise)')\n",
        "  plt.show()\n",
        "\n",
        "  #wdf[['reduced_symbols','realizedpnl']].reset_index().pivot_table(index = 'closedate',columns='reduced_symbols',values='realizedpnl')\n",
        "  #_tmp = wdf[['reduced_symbols','realizedpnl','Datewise']].groupby(['Datewise','reduced_symbols']).sum().reset_index().pivot(index='Datewise', columns='reduced_symbols', values='realizedpnl').replace({np.nan:0})\n",
        "  _tmp = wdf[['reduced_symbols','realizedpnl']].reset_index().groupby(['closedate','reduced_symbols']).sum().reset_index().pivot(index='closedate', columns='reduced_symbols', values='realizedpnl').replace({np.nan:0})\n",
        "  for col in _tmp.columns:\n",
        "    _tmp[col] = _tmp[col].cumsum()\n",
        "  #ax = _tmp.plot.bar(stacked=True, rot=45)\n",
        "  ax = _tmp.plot.area(stacked=False, rot=45)\n",
        "  wdf['realizedpnl'].cumsum().plot.line( linestyle = '-', linewidth = '2', style=['b--'], ax=ax)\n",
        "  plt.locator_params(axis='x', nbins=6)\n",
        "  plt.tick_params(\n",
        "      axis='x',          # changes apply to the x-axis\n",
        "      which='both',      # both major and minor ticks are affected\n",
        "      bottom=False,      # ticks along the bottom edge are off\n",
        "      top=False,         # ticks along the top edge are off\n",
        "      labelbottom=False) # labels along the bottom edge are off\n",
        "  #ax.set_yscale('symlog', linthreshy=600)\n",
        "  plt.grid()\n",
        "  plt.title('Cumulative PNL of Trades (Total and symbol-wise)')\n",
        "  plt.show()\n",
        "\n",
        "def PNL_Spread(wdf):\n",
        "  ax = wdf.plot.scatter(x=\"reduced_symbols\", y=\"realizedpnl\")\n",
        "  plt.grid(b=True, which='major')\n",
        "  ax.set_yscale('symlog', linthreshy=100)\n",
        "  plt.title('Realized_pnl Spread vs Symbols')\n",
        "  plt.show()\n",
        "\n",
        "def returns_indicator(wdf):\n",
        "  stat = wdf[['strategy','reduced_symbols','realizedpnl']].groupby(['strategy','reduced_symbols']).sum()\n",
        "  stat['color'] = stat['realizedpnl']\n",
        "  stat['size'] = np.sqrt(np.abs(stat['realizedpnl']))\n",
        "  while (stat['size'].max()<700):\n",
        "    stat['size'] = np.power(stat['size'],1.2)\n",
        "  stat['color'][stat['realizedpnl']>0] = 'Green'\n",
        "  stat['color'][stat['realizedpnl']<=0] = 'Red'\n",
        "  stat.reset_index(inplace=True)\n",
        "  stat.plot.scatter(x=\"reduced_symbols\", y=\"strategy\", c='color', s='size')\n",
        "  plt.title(\"Strategy-wise returns indicator\")\n",
        "  plt.show()\n",
        "  #stat\n",
        "\n",
        "  stat = wdf[['side','reduced_symbols','realizedpnl']].groupby(['side','reduced_symbols']).sum()\n",
        "  stat['color'] = stat['realizedpnl']\n",
        "  stat['size'] = np.sqrt(np.abs(stat['realizedpnl']))\n",
        "  while (stat['size'].max()<700):\n",
        "    stat['size'] = np.power(stat['size'],1.2)\n",
        "  stat['color'][stat['realizedpnl']>0] = 'Green'\n",
        "  stat['color'][stat['realizedpnl']<=0] = 'Red'\n",
        "  stat.reset_index(inplace=True)\n",
        "  stat.plot.scatter(x=\"reduced_symbols\", y=\"side\", c='color', s='size')\n",
        "  plt.title(\"Side-wise returns indicator\")\n",
        "  plt.show()\n",
        "\n",
        "def make_area_plots (DF, pos_neg=True):\n",
        "  wdf = DF.copy()\n",
        "  stkslcts = wdf.reduced_symbols.unique()\n",
        "  for stock in stkslcts:\n",
        "    cum_stocks = lambda x: x['realizedpnl'] if x['reduced_symbols']==stock else 0\n",
        "    r_stocks = lambda x: x['trade_rate'] if x['reduced_symbols']==stock else 0\n",
        "    pos_vals = lambda x: x['c-'+stock] if x['c-'+stock]>0 else 0\n",
        "    neg_vals = lambda x: x['c-'+stock] if x['c-'+stock]<0 else 0\n",
        "    wdf[stock] = wdf[['realizedpnl', 'reduced_symbols']].apply(cum_stocks, axis=1)\n",
        "    wdf['c-'+stock] = wdf[stock].cumsum()\n",
        "    wdf['r-'+stock] = wdf[['trade_rate', 'reduced_symbols']].apply(r_stocks, axis=1)\n",
        "    wdf['pos-'+stock] = wdf[['c-'+stock, 'reduced_symbols']].apply(pos_vals, axis=1)\n",
        "    wdf['neg-'+stock] = wdf[['c-'+stock, 'reduced_symbols']].apply(neg_vals, axis=1)\n",
        "  _tmp = ['pos-'+x for x in stkslcts]\n",
        "  wdf['pos_acm'] = wdf[_tmp].sum(axis=1)\n",
        "  _tmp = ['neg-'+x for x in stkslcts]\n",
        "  wdf['neg_acm'] = wdf[_tmp].sum(axis=1)\n",
        "  _tmp = ['neg-'+x for x in stkslcts]\n",
        "  wdf.set_index('closedate', inplace=True)\n",
        "\n",
        "  data = wdf[_tmp][wdf[_tmp].sum(axis=1)<0]\n",
        "  #plt.locator_params(axis='x', nbins=10)\n",
        "  #ax = data.plot.bar(stacked=True)\n",
        "  data_perc = data.divide(data.sum(axis=1), axis=0)\n",
        "  ax = data_perc.plot.area(stacked=True, alpha=0.7)\n",
        "  wdf[['neg_acm']].plot(kind='line', secondary_y=True, linestyle = '-', linewidth = '2', style=['r--'], ax=ax)\n",
        "  plt.title('Negative Area Plot')\n",
        "  plt.show()\n",
        "\n",
        "  _tmp = ['pos-'+x for x in stkslcts]\n",
        "  data = wdf[_tmp][wdf[_tmp].sum(axis=1)>0]\n",
        "  data_perc = data.divide(data.sum(axis=1), axis=0)*-1\n",
        "  ax = data_perc.plot.area(stacked=True)\n",
        "  wdf[['pos_acm']].plot(kind='line', secondary_y=True, linestyle = '-', linewidth = '2', style=['b--'], ax=ax)\n",
        "  plt.title('Positive Area Plot')\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CIDBqlM5OzI8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Plot Functions"
      ],
      "metadata": {
        "id": "uYvpTwllnoUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_parallel(df_stkLim, rdims):\n",
        "  try:\n",
        "    #cols = [color_col]+list(dims)\n",
        "    cols = list(rdims)\n",
        "    filt = df_stkLim[list(filter(None, cols))].copy()\n",
        "    #filt = filter_nans(self.df_stkLim[list(filter(None, cols))].copy())\n",
        "    #fig = px.parallel_coordinates(filt, color=color_col, dimensions=list(dims), width=1600, height=800)\n",
        "    fig = px.parallel_coordinates(filt, dimensions=list(rdims))#, width=1600, height=800)\n",
        "    fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "    return fig\n",
        "    display(HTML(fig.to_html()))\n",
        "  except Exception as E:\n",
        "    raise E\n",
        "    print(\"Couldn't make graph due to following error: \"), print(E)\n",
        "\n",
        "def make_scatter(df_stkLim, xaxis,yaxis=None,size_col=None,color_col=None, symbol_col=None, facet_col=None, hover_col=None):\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    cols = [xaxis, yaxis, size_col, color_col, symbol_col, facet_col, hover_col]\n",
        "    col2 = [xaxis, yaxis, color_col, symbol_col, facet_col, hover_col]\n",
        "    #try:\n",
        "    if (size_col in col2) and (size_col is not None):\n",
        "      filt = filter_nans(df_stkLim[list(filter(None, col2))].copy())\n",
        "      filt[size_col+'_'] = filt[size_col] \n",
        "      size_col = size_col+'_'\n",
        "    else:\n",
        "      #pass\n",
        "      #filt = df_stkLim\n",
        "      filt = filter_nans(df_stkLim[list(filter(None, cols))].copy())\n",
        "    #filt = filt.groupby([xaxis,yaxis]).agg({size_col:'sum', color_col:'min', symbol_col:'min', facet_col:'min', hover_col:'min'})\n",
        "    \n",
        "    agg_cols = [size_col, color_col, symbol_col, facet_col, hover_col]\n",
        "    agg_mets = ['sum', 'first', 'first', 'first', 'first']\n",
        "    agg_dict={}\n",
        "    \n",
        "    for i in range(len(agg_mets)):\n",
        "      if agg_cols[i] is not None:\n",
        "        agg_dict[agg_cols[i]] = agg_mets[i]\n",
        "    if agg_dict != {}:\n",
        "      filt = filt.groupby([xaxis,yaxis]).agg(agg_dict)\n",
        "    try:\n",
        "      filt[size_col] = np.abs(filt[size_col])\n",
        "      filt.rename(columns={size_col: 'ABS('+size_col+')'}, inplace=True)\n",
        "      size_col = 'ABS('+size_col+')'\n",
        "    except:\n",
        "      size_col = None\n",
        "    filt = filter_nans(filt)\n",
        "    fig = px.scatter(filt.reset_index(), x=xaxis, y=yaxis, color=color_col, size=size_col, symbol=symbol_col, facet_col=facet_col, hover_data=[hover_col])#, width=1600, height=800)\n",
        "    fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "    return fig\n",
        "    #display(HTML(fig.to_html()))\n",
        "    ##resize_colab_cell()\n",
        "    display(HTML(fig.to_html()))\n",
        "  except Exception as E:\n",
        "    raise E\n",
        "    print(\"Couldn't make graph due to following error: \"), print(E)\n",
        "\n",
        "\n",
        "def make_3dscatter(df_stkLim, xaxis, yaxis, zaxis, size_col=None, color_col=None, symbol_col=None, hover_col=None):\n",
        "  try:\n",
        "    cols = [xaxis, yaxis, zaxis, size_col, color_col, symbol_col, hover_col]\n",
        "    col2 = [xaxis, yaxis, zaxis, color_col, symbol_col, hover_col]\n",
        "    #try:\n",
        "    if (size_col in col2) and (size_col is not None):\n",
        "      filt = filter_nans(df_stkLim[list(filter(None, col2))].copy())\n",
        "      filt[size_col+'_'] = filt[size_col] \n",
        "      size_col = size_col+'_'\n",
        "    else:\n",
        "      filt = filter_nans(df_stkLim[list(filter(None, cols))].copy())\n",
        "    #filt = filt.groupby([xaxis,yaxis]).agg({size_col:'sum', color_col:'min', symbol_col:'min', hover_col:'min'})\n",
        "    agg_cols = [size_col, color_col, symbol_col, hover_col]\n",
        "    agg_mets = ['sum', 'first', 'first', 'first']\n",
        "    agg_dict={}\n",
        "    for i in range(len(agg_mets)):\n",
        "      if agg_cols[i] is not None:\n",
        "        agg_dict[agg_cols[i]] = agg_mets[i]\n",
        "    if agg_dict != {}:\n",
        "      filt = filt.groupby([xaxis,yaxis,zaxis]).agg(agg_dict)\n",
        "    try:\n",
        "      filt[size_col] = np.abs(filt[size_col])\n",
        "      filt.rename(columns={size_col: 'ABS('+size_col+')'}, inplace=True)\n",
        "      size_col = 'ABS('+size_col+')'\n",
        "    except:\n",
        "      size_col = None\n",
        "    filt = filter_nans(filt)\n",
        "    fig = px.scatter_3d(filt.reset_index(), x=xaxis, y=yaxis, z=zaxis, color=color_col, size=size_col, symbol=symbol_col, hover_data=[hover_col])#, width=1600, height=800)\n",
        "    fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "    return fig\n",
        "    #display(HTML(fig.to_html()))\n",
        "    ##resize_colab_cell()\n",
        "    display(HTML(fig.to_html()))\n",
        "  except Exception as E:\n",
        "    raise E\n",
        "    print(\"Couldn't make graph due to following error: \"), print(E)\n",
        "\n",
        "def make_line(df_stkLim, xaxis, yaxis, color_col=None):\n",
        "  try:\n",
        "    cols = [xaxis, yaxis, color_col]\n",
        "    filt = filter_nans(df_stkLim[list(filter(None, cols))].copy())\n",
        "    filt = filter_nans(filt)\n",
        "    if color_col:\n",
        "      filt = filt.groupby([xaxis,color_col]).agg('sum')\n",
        "    else:\n",
        "      filt = filt.groupby([xaxis]).agg('sum')\n",
        "    filt = filter_nans(filt)\n",
        "    filt.reset_index(inplace=True)\n",
        "    filt = filt.sort_values(by=xaxis)\n",
        "    if color_col:\n",
        "      filt = filt.sort_values(by=color_col)\n",
        "    filt = filt.sort_values(xaxis).reset_index(drop=True)\n",
        "    if xaxis in line_x_axis_cat:\n",
        "      fig = px.scatter(filt, x=xaxis, y=yaxis, color=color_col)#, width=1600, height=800)\n",
        "    else:\n",
        "      fig = px.line(filt, x=xaxis, y=yaxis, color=color_col)#, width=1600, height=800)\n",
        "    fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "    return fig\n",
        "    ##resize_colab_cell()\n",
        "    display(HTML(fig.to_html()))\n",
        "    \n",
        "  except Exception as E:\n",
        "    raise E\n",
        "    print(\"Couldn't make graph due to following error: \"), print(E)\n",
        "def make_treemap_anim(df_stkLim, out=None):\n",
        "  def make_treemap_params(row):\n",
        "    pos_sum = 0\n",
        "    neg_sum = 0\n",
        "    try:\n",
        "      period_string = 'Year:'+row[list(row.keys())[0]][:4]+' Month:'+row[list(row.keys())[0]][4:6]+' Week:'+row[list(row.keys())[0]][6:]\n",
        "    except:\n",
        "      period_string = row[list(row.keys())[0]]\n",
        "    param_dict = {'parents':[], 'labels':[],'values':[], 'Period':[period_string]}\n",
        "    for key in row['realizedpnl'].keys():\n",
        "      param_dict['labels'] += [key]\n",
        "      if row['realizedpnl'][key] >= 0:\n",
        "        param_dict['parents'] += ['gains']\n",
        "        param_dict['values'] += [row['realizedpnl'][key]]\n",
        "        pos_sum += row['realizedpnl'][key]\n",
        "      else:\n",
        "        param_dict['parents'] += ['losses']\n",
        "        param_dict['values'] += [row['realizedpnl'][key] * (-1)]\n",
        "        neg_sum += (-1)*row['realizedpnl'][key]\n",
        "    param_dict['labels'] = ['gains', 'losses'] + param_dict['labels']\n",
        "    param_dict['parents'] = [param_dict['Period'],param_dict['Period']] + param_dict['parents']\n",
        "    param_dict['values'] = [pos_sum,neg_sum] + param_dict['values']\n",
        "    return param_dict\n",
        "\n",
        "\n",
        "  period = 'YMW'\n",
        "  tmp_data = df_stkLim[[period, 'reduced_symbols', 'realizedpnl']].copy()\n",
        "  tmp_data = tmp_data.groupby([period,'reduced_symbols']).agg('sum').unstack(level=-1).fillna(0).cumsum(axis=0).reset_index()\n",
        "  tmp_data ['TM_params'] = tmp_data.apply(make_treemap_params, axis=1)\n",
        "  tmp_data\n",
        "\n",
        "  proc_data = tmp_data['TM_params']\n",
        "  proc_frames = [go.Frame(data=[go.Treemap(labels = proc_data[i]['labels'], parents = proc_data[i]['parents'], values =  proc_data[i]['values'],root_color=\"lightgrey\",branchvalues = \"total\"\n",
        "  ,textinfo = \"label+value+percent parent+percent entry+percent root\"  )])\n",
        "                  for i in range(1,len(proc_data))]\n",
        "  button_styles = {\"buttons\": \n",
        "                    [ {\"args\": \n",
        "                      [None, {\"frame\": {\"duration\": 500, \"redraw\": False},\n",
        "                              \"fromcurrent\": True,\n",
        "                              \"transition\": {\"duration\": 300, \"easing\": \"quadratic-in-out\"}}],\n",
        "                      \"label\": \"Play\",\n",
        "                      \"method\": \"animate\" },\n",
        "                    {\"args\":\n",
        "                      [[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n",
        "                                \"mode\": \"immediate\",\n",
        "                                \"transition\": {\"duration\": 0}}],\n",
        "                      \"label\": \"Pause\",\n",
        "                      \"method\": \"animate\"}],\n",
        "                    \"direction\": \"left\",\n",
        "                    \"pad\": {\"r\": 10, \"t\": 87},\n",
        "                    \"showactive\": False,\n",
        "                    \"type\": \"buttons\",\n",
        "                    \"x\": 0.11,\n",
        "                    \"xanchor\": \"right\",\n",
        "                    \"y\": 0.0,\n",
        "                    \"yanchor\": \"top\"}\n",
        "  fig = go.Figure(\n",
        "    data=[\n",
        "      go.Treemap(marker_colors = [px.colors.qualitative.Plotly[0], px.colors.qualitative.Plotly[1]],\n",
        "        labels = proc_data[0]['labels'],\n",
        "        parents = proc_data[0]['parents'],\n",
        "        values =  proc_data[0]['values'],\n",
        "        #textinfo = \"label+value+percent parent+percent entry+percent root\",\n",
        "        root_color=\"lightgrey\")\n",
        "    ],\n",
        "    layout=\n",
        "      go.Layout(\n",
        "          #width=1200, height=800,\n",
        "          #title=\"Start Title\",\n",
        "          updatemenus=[button_styles]\n",
        "      )\n",
        "    ,\n",
        "    frames=proc_frames\n",
        "  )\n",
        "\n",
        "  fig.update_layout(treemapcolorway = [px.colors.qualitative.Plotly[0], px.colors.qualitative.Plotly[1]], margin = dict(t=50, l=25, r=25, b=25))\n",
        "  fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "  return fig\n",
        "  with out:\n",
        "    display(HTML(fig.to_html()))\n",
        "    #fig.show()\n",
        "  "
      ],
      "metadata": {
        "id": "4d0cRmJLnrur"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Graph UI Functions"
      ],
      "metadata": {
        "id": "pYgpn2Ornvbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def make_initial_UI(self, EH_function, nested = None):\n",
        "    resize_colab_cell()\n",
        "    selector_options_s = ['All', 'Most Traded', 'Most Impactful', 'Safest', 'Worst', 'Quick Rising'] + self.df_full.reduced_symbols.unique().tolist()\n",
        "    selector_options = ['Cumulative Plots', 'Time Distribution Plots', 'PNL Spread vs Symbols', 'Returns Indicator Plots', 'Profit/Loss Area Plots', \"Interactive Scatter Plot\", \"Interactive 3D Scatter Plot\", \"Interactive Aggregate Plot\", \"Interactive Parallel Plot\", \"Treemap Animation\"]\n",
        "    # Defining Widgets\n",
        "    radbt = widgets.RadioButtons(options=['All Data', 'Only Live', 'Only Backtest'], value='All Data', description='Select Data', disabled=False)\n",
        "    date1 = widgets.DatePicker(value=self.df_full['E_Dates'].iloc[0])\n",
        "    date2 = widgets.DatePicker(value=self.df_full['E_Dates'].iloc[-1])\n",
        "    slctr = widgets.SelectMultiple(options=selector_options_s, description='Stocks:', disabled=False, value = (selector_options_s[0],) )\n",
        "    chkbx = widgets.Checkbox( value=False, description='Filter outlier trade entries?', disabled=False)\n",
        "    embbt = widgets.ToggleButtons( options=selector_options, description='Plot Selection:', disabled=False, button_style='', tooltips=selector_options, value=selector_options[5] )\n",
        "\n",
        "    # Combine the widgets into a single display layout\n",
        "    form_item_layout = Layout(display='flex', justify_content='space-between')\n",
        "    dates = HBox([Label(value='',layout=Layout(width='60px')),Label(value='Date Selection:'),VBox([date1,date2])])\n",
        "    data_opts = VBox([radbt,chkbx], layout=form_item_layout)\n",
        "    row2 = HBox([data_opts, slctr, dates])\n",
        "    grid = VBox([embbt, row2], layout=form_item_layout)\n",
        "\n",
        "    # Define one single output widget\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Define only one event handler that will be called for each widget\n",
        "    def internal_event_handler(change):\n",
        "      # The called function must have knowledge of which inputs to expect\n",
        "      output.clear_output()\n",
        "      if self.cur_output is not None:\n",
        "        clear_output()\n",
        "        display(grid)\n",
        "        display(output)\n",
        "      EH_function(output, [radbt.value, date1.value, date2.value, slctr.value, chkbx.value, embbt.value])\n",
        "    \n",
        "    # Attach the event handler to all widgets using their respective .observe methods\n",
        "    radbt.observe(internal_event_handler, names='value')\n",
        "    date1.observe(internal_event_handler, names='value')\n",
        "    date2.observe(internal_event_handler, names='value')\n",
        "    slctr.observe(internal_event_handler, names='value')\n",
        "    chkbx.observe(internal_event_handler, names='value')\n",
        "    embbt.observe(internal_event_handler, names='value')\n",
        "\n",
        "    # Display the output and layouts\n",
        "    # If nested, use the display from previous widget\n",
        "    if nested is not None:\n",
        "      with nested:\n",
        "        display(grid)\n",
        "        display(output)\n",
        "        with output:\n",
        "          # initialize the internal output\n",
        "          print(\"\")\n",
        "    else:\n",
        "      display(grid)\n",
        "      display(output)\n",
        "      with output:\n",
        "        # initialize the internal output\n",
        "        print(\"\")\n",
        "    # Call internal function to initiliaze proper state\n",
        "    internal_event_handler(None)\n",
        "\n",
        "    # return both display items for further use\n",
        "    return grid, output\n"
      ],
      "metadata": {
        "id": "UxBVN6ountYG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preproc Functions"
      ],
      "metadata": {
        "id": "kL8aiObDoqvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file(filename=''):\n",
        "  if os.path.exists(filename):\n",
        "    df_inp = pd.read_csv(filename)\n",
        "  else:\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    df_inp = pd.read_csv(filename)\n",
        "  return df_inp\n",
        "\n",
        "\n",
        "def preprocess_df(df_inp, trades_filter=10):\n",
        "  df_full = pd.DataFrame()\n",
        "  df_liveLim = pd.DataFrame()\n",
        "  df_dateLim = pd.DataFrame()\n",
        "\n",
        "  dates_range = []\n",
        "  selector = None\n",
        "\n",
        "  persistent_plot_params=[]\n",
        "  persistent_chk = ''\n",
        "\n",
        "  json_column_keys_dict = {}\n",
        "  json_options = []\n",
        "  global global_json_var\n",
        "\n",
        "  cur_output = None\n",
        "\n",
        "  debug = None\n",
        "  # Convert date columns to pandas datetime format and EST\n",
        "  date_cols = ['entrydate', 'closedate']\n",
        "  eastern = pytz.timezone('US/Eastern')\n",
        "  df_inp[date_cols[0]], df_inp[date_cols[1]] = pd.to_datetime(df_inp[date_cols[0]], utc=True).dt.tz_convert(eastern), pd.to_datetime(df_inp[date_cols[1]], utc=True).dt.tz_convert(eastern)\n",
        "\n",
        "  # Make an int column to use as reference index \n",
        "  df_inp['idxdate'] = df_inp['entrydate'].dt.strftime(\"%y%m%d%H%M%S\").astype(int)\n",
        "  df_inp.sort_values('idxdate', inplace=True)\n",
        "  df_inp.set_index('idxdate', inplace=True)\n",
        "\n",
        "  ss_i = 0\n",
        "  # Clean the symbol column\n",
        "  df_inp['reduced_symbols'] = df_inp['symbol'].apply(clean_symbols)\n",
        "\n",
        "  # Replace less-traded stocks as 'OTHER'\n",
        "  df_inp['full_symbols'] = df_inp['reduced_symbols']  # Keep a copy of the column\n",
        "  _tmp = df_inp['reduced_symbols'].value_counts().reset_index()\n",
        "  other_stocks = _tmp[_tmp['reduced_symbols']<trades_filter]['index'].unique().tolist()\n",
        "  for stk in other_stocks:\n",
        "    df_inp['reduced_symbols'].replace({stk: 'OTHER'}, inplace=True)\n",
        "  \n",
        "  # Add time-in-trade column and handle faulty rows, replacing Faulty Values with default (1) or removing the respective rows\n",
        "  df_inp['time-in-trade'] = (df_inp['closedate'] - df_inp['entrydate']).astype('timedelta64[s]')\n",
        "  df_inp = df_inp[df_inp['time-in-trade'] > 0]\n",
        "  #df_inp['time-in-trade'][ df_inp['time-in-trade']<=0 ] = 1\n",
        "\n",
        "  # Make a working copy, and delete extra columns in inp to make total<=20 (for colab display)\n",
        "  df_full = df_inp.copy()\n",
        "  df_inp.drop('symbol', inplace=True, axis=1)\n",
        "  df_inp.drop('full_symbols', inplace=True, axis=1)\n",
        "\n",
        "  # Add a rates and win/loss columns\n",
        "  df_full['trade_rate'] = df_full[['realizedpnl', 'time-in-trade']].apply(rate_calc, axis=1)\n",
        "  df_full['W/L'] = df_full['realizedpnl'].apply(win_chk)\n",
        "\n",
        "  # Convert to long/short sides\n",
        "  df_full['Long/Short Trades'] = df_full['side'].apply(LScnv)\n",
        "\n",
        "  \n",
        "  # Add additional columns for later grouping and filtering operations\n",
        "  df_full['E_Dates'] = df_full['entrydate'].dt.date#.apply(converter)\n",
        "  df_full['Datewise'] = df_full['closedate'].dt.date#.apply(converter)\n",
        "  df_full['C_Time'] = df_full['closedate'].dt.time\n",
        "  df_full['H'] = df_full['entrydate'].dt.strftime(\"%H\").astype(int)\n",
        "  df_full['D'] = df_full['entrydate'].dt.strftime(\"%d\").astype(int)\n",
        "  df_full['M'] = df_full['entrydate'].dt.strftime(\"%m\").astype(int)\n",
        "  df_full['hm'] = df_full['entrydate'].dt.strftime(\"%H%M\"[:4]).astype(int)\n",
        "  df_full['hms'] = df_full['entrydate'].dt.strftime(\"%H:%M:%S\")\n",
        "  df_full['YM'] = df_full['entrydate'].dt.strftime(\"%Y%m\")\n",
        "  df_full['YMW'] = df_full['entrydate'].dt.strftime(\"%Y%m%W\")\n",
        "  df_full['Day of Week'] = df_full['entrydate'].dt.day_name()\n",
        "  df_full['time_bracket'] = df_full['hm'].apply(time_bins)\n",
        "  df_full['time_bracket_lim'] = df_full['hm'].apply(time_bins_lim)\n",
        "  df_full['Count'] = 1\n",
        "\n",
        "  # Translate JSON columns to respective details\n",
        "  \n",
        "  set_var()\n",
        "  df_full['lifecycle_det'] = df_full['lifecycle'].apply(lambda x: np.nan if x is np.nan else extract_leaves(x))\n",
        "  json_column_keys_dict['lifecycle'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'lifecycle_det', json_column_keys_dict['lifecycle'])\n",
        "  \n",
        "\n",
        "  set_var()\n",
        "  df_full['setup_type'] = df_full['setup'].apply(lambda x: np.nan if x is np.nan else extract_roots(x))\n",
        "  df_full['setup_det'] = df_full['setup'].apply(lambda x: np.nan if x is np.nan else extract_leaves(x, filters=['pattern']))\n",
        "  json_column_keys_dict['setup'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'setup_det', json_column_keys_dict['setup'])\n",
        "\n",
        "  set_var()\n",
        "  df_full['setupFilters_type'] = df_full['setupFilters'].apply(lambda x: np.nan if x is np.nan else extract_roots(x))\n",
        "  df_full['setupFilters_det'] = df_full['setupFilters'].apply(lambda x: np.nan if x is np.nan else extract_leaves(x))\n",
        "  json_column_keys_dict['setupFilters'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'setupFilters_det', json_column_keys_dict['setupFilters'])\n",
        "\n",
        "  set_var()\n",
        "  df_full['options_det'] = df_full['options'].apply(lambda x: np.nan if x is np.nan else clean_options(extract_leaves(x)))\n",
        "  json_column_keys_dict['options'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'options_det', json_column_keys_dict['options'])\n",
        "\n",
        "  set_var()\n",
        "  df_full['entries_type'] = df_full['entries'].apply(lambda x: np.nan if x is np.nan else extract_roots(x))\n",
        "  df_full['entries_det'] = df_full['entries'].apply(lambda x: np.nan if x is np.nan else extract_leaves(x))\n",
        "  json_column_keys_dict['entries'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'entries_det', json_column_keys_dict['entries'])\n",
        "\n",
        "  set_var()\n",
        "  df_full['exits_types'] = df_full['exits'].apply(lambda x: np.nan if x is np.nan else extract_roots(x))\n",
        "  df_full['exits_dict'] = df_full['exits'].apply(lambda x: np.nan if x is np.nan else extract_leaves(x, filters=['paused', 'tacticId']))\n",
        "  json_column_keys_dict['exits'] = get_var().copy()\n",
        "  df_full = expand_into_columns(df_full, 'exits_dict', json_column_keys_dict['exits'])\n",
        "\n",
        "  for key in json_column_keys_dict.keys():\n",
        "    json_options += json_column_keys_dict[key]\n",
        "  \n",
        "  # Set the variable defining possible range of date selection\n",
        "  dates_range = df_full.E_Dates.unique()\n",
        "\n",
        "\n",
        "  return df_full, df_inp, dates_range, json_column_keys_dict, json_options"
      ],
      "metadata": {
        "id": "rsucof2BiYdC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WDF Functions"
      ],
      "metadata": {
        "id": "mVWlef-AFr6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_wdf(df_base, opts):\n",
        "  #opts = json.loads(opts)\n",
        "  start, end, Liv, Bck, Flt = opts\n",
        "  start = datetime.datetime.strptime(str(start), '%Y-%m-%d').date()\n",
        "  end = datetime.datetime.strptime(str(end), '%Y-%m-%d').date()\n",
        "  if Liv:\n",
        "    if Bck:\n",
        "      bckTest='All Data'\n",
        "    else:\n",
        "      bckTest='Only Live'\n",
        "  else:\n",
        "    if Bck:\n",
        "      bckTest='Only Bck'\n",
        "    else:\n",
        "      bckTest='All Data'\n",
        "  if bckTest == 'All Data':\n",
        "    wdf = df_base.copy()\n",
        "  elif bckTest == 'Only Live':\n",
        "    wdf = df_base[df_base['accountId']!='BACKTEST'].copy()\n",
        "  elif bckTest == 'Only Bck':\n",
        "    wdf = df_base[df_base['accountId']=='BACKTEST'].copy()\n",
        "  else:\n",
        "    wdf = df_base.copy()\n",
        "  \n",
        "  try:\n",
        "    if (start>wdf['E_Dates'].iloc[-1]) or (end<wdf['E_Dates'].iloc[0]) or (start>end):\n",
        "      pass\n",
        "  except:\n",
        "    wdf.E_Dates = pd.to_datetime(wdf.E_Dates, format='%Y-%m-%d').dt.date\n",
        "  if (start>wdf['E_Dates'].iloc[-1]) or (end<wdf['E_Dates'].iloc[0]) or (start>end):\n",
        "    pass\n",
        "  else:\n",
        "    # find start/end indices\n",
        "    i_start, i_end = np.where(wdf.E_Dates >= start)[0][0], np.where(wdf.E_Dates <= end)[0][-1]\n",
        "    \n",
        "    if (i_start >= i_end):\n",
        "      pass\n",
        "    else:\n",
        "      \n",
        "      # apply indices\n",
        "      wdf = wdf.iloc[i_start:i_end].copy()\n",
        "      \n",
        "      # Recalculate cumulative pnl\n",
        "      wdf['pnl_accumulated'] = wdf.realizedpnl.cumsum()\n",
        "      #print(\"\\rStart Date: {}\\t\\tEnd Date: {}\".format(i_start,i_end), end=\"\\n\")\n",
        "  if Flt:\n",
        "    wdf = filter_outliers(wdf.copy(), columns_to_check = ['realizedpnl', 'time-in-trade', 'trade_rate'])\n",
        "    #print('Filtered outliers in {}'.format(['realizedpnl', 'time-in-trade', 'trade_rate']))\n",
        "  #print(limit_list)\n",
        "  \n",
        "  wdf['pnl_accumulated'] = wdf.realizedpnl.cumsum()\n",
        "  \n",
        "  return wdf"
      ],
      "metadata": {
        "id": "zJsCRT2tFsgf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dataframes"
      ],
      "metadata": {
        "id": "RbiudwuKhyKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df_inpp = upload_file('lim_cprd.csv')\n",
        "#json_txt = df_inpp.iloc[list(range(19,31)),:].to_json(date_format='iso', orient='split')\n",
        "#df_inp = pd.read_json(json_txt, orient='split')\n",
        "#df_inp\n",
        "#df_inp = upload_file('lim_cprd.csv')\n",
        "df_inp_txt = '{\"columns\":[\"symbol\",\"side\",\"realizedpnl\",\" percent_pnl\",\"costbasis\",\"quantity\",\"entrydate\",\"closedate\",\"strategy\",\"strategydescription\",\"tradePlanTemplate\",\"accountId\",\"day\",\"lifecycle\",\"setup\",\"setupFilters\",\"options\",\"entries\",\"exits\"],\"index\":[19,20,21,22,23,24,25,26,27,28,29,30],\"data\":[[\"GME\",\"buy\",-1.22,-0.086043862,1417.8815,7,\"2021-06-25T11:14:05.000-0400\",\"2021-06-25T11:14:07.000-0400\",\"AD_HOC\",\"AD_HOC\",\"Sim on Live data\",\"BACKTEST\",\"25\\/06\\/2021\",null,null,null,null,null,null],[\"GME\",\"sell_short\",-6.67,-0.133949192,4979.5,23,\"2021-06-24T12:30:07.000-0400\",\"2021-06-24T12:30:10.000-0400\",\"CANDLE_PATTERN\",\"INSIDE_CANDLE-BREAK_HIGH-BREAK_LOW\",\"1-2-2 S sim\",\"BACKTEST\",\"24\\/06\\/2021\",null,null,null,null,null,null],[\"SCR\",\"sell_short\",-8.16,-0.032744415,24920.28,1023,\"2021-06-23T09:38:23.000-0400\",\"2021-06-23T09:39:11.000-0400\",\"ORB\",null,null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"AMC\",\"buy\",312.72,1.252923769,24959.22,429,\"2021-06-23T09:35:37.000-0400\",\"2021-06-23T09:38:43.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"ABCL\",\"sell_short\",26.95,0.108213983,24904.36,1222,\"2021-06-23T09:36:50.000-0400\",\"2021-06-23T09:38:39.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"TSLA\",\"buy\",132.81,0.532186974,24955.515,39,\"2021-06-23T09:35:07.000-0400\",\"2021-06-23T09:38:02.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"GME\",\"sell_short\",-138.51,-0.556815838,24875.37,114,\"2021-06-23T09:37:12.000-0400\",\"2021-06-23T09:37:55.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"CLOV\",\"sell_short\",-47.58,-0.190755686,24942.9,1830,\"2021-06-23T09:37:11.000-0400\",\"2021-06-23T09:37:26.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"BA\",\"sell_short\",-63.24,-0.25383828,24913.5,102,\"2021-06-23T09:35:01.000-0400\",\"2021-06-23T09:35:45.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"LOOP\",\"buy\",213.78,0.852249623,25084.2,1724,\"2021-06-23T09:35:04.000-0400\",\"2021-06-23T09:35:36.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"ENTX\",\"sell_short\",-109.95,-0.444444444,24738.75,3665,\"2021-06-23T09:35:19.000-0400\",\"2021-06-23T09:35:20.000-0400\",\"ORB\",\"ORB\",null,\"BACKTEST\",\"23\\/06\\/2021\",null,null,null,null,null,null],[\"MRNA\",\"buy\",-26.55,-1.856968001,1429.75,7,\"2021-06-18T09:41:33.000-0400\",\"2021-06-18T10:46:29.000-0400\",\"CANDLE_TRIGGER\",\"CANDLE_TRIGGER\",null,\"BACKTEST\",\"18\\/06\\/2021\",null,null,null,null,null,null]]}'\n",
        "df_inp = pd.read_json(df_inp_txt, orient = 'split')\n",
        "df_full, df_inp, dates_range, json_column_keys_dict, json_options = preprocess_df(df_inp)"
      ],
      "metadata": {
        "id": "vH7aqBlkw8h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e752ac1-4b28-4f05-cbb7-7513166c47c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = make_parallel(df_full, result_cats)\n",
        "sctr = make_scatter(df_full, 'realizedpnl', ' percent_pnl')\n",
        "sctr3d = make_3dscatter(df_full, 'realizedpnl', ' percent_pnl', 'quantity')\n",
        "aggr = make_line(df_full, 'realizedpnl', ' percent_pnl')\n",
        "tmap = make_treemap_anim(df_full)"
      ],
      "metadata": {
        "id": "ZSRkabf0y2IE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_date = df_full['E_Dates'].iloc[0]\n",
        "en_date = df_full['E_Dates'].iloc[-1]\n",
        "df_full = df_full.iloc[:100,:]"
      ],
      "metadata": {
        "id": "vNg3yLre08xk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Interface"
      ],
      "metadata": {
        "id": "eIygdtEH8Cqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
        "\n",
        "\n",
        "bg_color = '#b0c4de'\n",
        "fig = go.Figure()\n",
        "\n",
        "#style_main = { 'height': '800px', 'width': '1600px', 'background-color':bg_color, 'margin': '0'}\n",
        "style_main = { 'height': '800px', 'width': '1600px', 'background-color':bg_color, 'margin': '0', 'display':'block'}\n",
        "style_left_pane = { 'height': '100%', 'width': '22%', 'background-color':bg_color, 'float': 'left',  'margin-left':'2%'}\n",
        "style_right_pane = { 'height': '99%', 'width': '72%', 'background-color':bg_color, 'float': 'right', 'margin-right':'2%' }\n",
        "style_plot_options = { 'height': '50%', 'width': '100%', 'background-color':bg_color, 'margin': '0', 'float':'bottom' , 'overflow': 'hidden', 'position': 'relative'}\n",
        "style_graph = { 'height': '50%', 'width': '100%', 'background-color':bg_color, 'margin': '0', 'float':'top' }\n",
        "\n",
        "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
        "du.configure_upload(app, r\"./content/uploads/\")#, use_upload_id=True)\n",
        "scatter_UI = html.Div([\n",
        "  html.Div(['xaxis'     , dcc.Dropdown(axis_options, axis_options[0], id='sui_xaxis'     , clearable = False, searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['yaxis'     , dcc.Dropdown(axis_options, axis_options[1], id='sui_yaxis'     , clearable = False, searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['size_col'  , dcc.Dropdown(size_options, id='sui_size_col'  , clearable = True , searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['color_col' , dcc.Dropdown(ctgr_options, id='sui_color_col' , clearable = True , searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['symbol_col', dcc.Dropdown(symb_options, id='sui_symbol_col', clearable = True , searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['facet_col' , dcc.Dropdown(fcet_options, id='sui_facet_col' , clearable = True , searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['hover_col' , dcc.Dropdown(hver_options, id='sui_hover_col' , clearable = True , searchable=True, className='sui')], style = {'width':'300px', 'float': 'left'})\n",
        "], id='sui_div', style={'display':'none'})\n",
        "\n",
        "scatter3d_UI = html.Div([\n",
        "  html.Div(['xaxis'     , dcc.Dropdown(axis_options, axis_options[0], id='sui3d_xaxis'     , clearable = False, searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['yaxis'     , dcc.Dropdown(axis_options, axis_options[1], id='sui3d_yaxis'     , clearable = False, searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['zaxis'     , dcc.Dropdown(axis_options, axis_options[2], id='sui3d_zaxis'     , clearable = False, searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['size_col'  , dcc.Dropdown(size_options, id='sui3d_size_col'  , clearable = True , searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['color_col' , dcc.Dropdown(ctgr_options, id='sui3d_color_col' , clearable = True , searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['symbol_col', dcc.Dropdown(symb_options, id='sui3d_symbol_col', clearable = True , searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['hover_col' , dcc.Dropdown(hver_options, id='sui3d_hover_col' , clearable = True , searchable=True, className='sui3d')], style = {'width':'300px', 'float': 'left'})\n",
        "], id='3dui_div', style={'display':'none'})\n",
        "\n",
        "aggr_UI = html.Div([\n",
        "  html.Div(['xaxis'     ,dcc.Dropdown(line_x_axis, line_x_axis[0], id='aggr_xaxis'     , clearable = False, searchable=True, className='aggr')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['yaxis'     ,dcc.Dropdown(line_y_axis, line_y_axis[1], id='aggr_yaxis'     , clearable = False, searchable=True, className='aggr')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['color_col' ,dcc.Dropdown(line_colors, id='aggr_color_col' , clearable = True, searchable=True, className='aggr')], style = {'width':'300px', 'float': 'left'})\n",
        "], id='aggr_div', style={'display':'none'})\n",
        "\n",
        "para_UI = html.Div([\n",
        "  html.Div(['result_cats'     , dcc.Dropdown(result_cats                           , id='para_result_cats'   , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['lifecycle'       , dcc.Dropdown(json_column_keys_dict['lifecycle']    , id='para_lifecycle'     , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['setup'           , dcc.Dropdown(json_column_keys_dict['setup']        , id='para_setup'         , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['setupFilters'    , dcc.Dropdown(json_column_keys_dict['setupFilters'] , id='para_setupFilters'  , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['options'         , dcc.Dropdown(json_column_keys_dict['options']      , id='para_options'       , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['entries'         , dcc.Dropdown(json_column_keys_dict['entries']      , id='para_entries'       , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'}),\n",
        "  html.Div(['exits'           , dcc.Dropdown(json_column_keys_dict['exits']        , id='para_exits'         , multi = True, clearable = True, searchable=True, className='para')], style = {'width':'300px', 'float': 'left'})\n",
        "], id='para_div', style={'display':'none'})\n",
        "\n",
        "\n",
        "app.layout = html.Div([  #html.Div([  dcc.Upload(html.Button('Upload File', style={'aligned':'center'}), id='upload_file')  ], id='upl_Container'),\n",
        "html.Div([\n",
        "  dcc.Store(id='graph_sel', data = json.dumps('sctr')),\n",
        "  dcc.Store(id='sui_state', data = json.dumps([axis_options[0], axis_options[1], None, None, None, None, None])),\n",
        "  dcc.Store(id='sui3d_state', data = json.dumps([axis_options[0], axis_options[1], axis_options[2], None, None, None, None])),\n",
        "  dcc.Store(id='aggr_state', data = json.dumps([line_x_axis[0], line_y_axis[1], None])),\n",
        "  dcc.Store(id='para_state', data = json.dumps(result_cats)),\n",
        "  dcc.Store(id='options_state'),\n",
        "  dcc.Store(id='sdf'),#, data=df_full.to_json(date_format='iso', orient='split')),\n",
        "  dcc.Store(id='upload_status', data=json.dumps('Failure')),\n",
        "  dcc.Store(id='upload_path', data=json.dumps('/N/A/')),\n",
        "  dcc.Store(id='mem_df', data=df_full.to_json(date_format='iso', orient='split')),\n",
        "  \n",
        "\n",
        "  html.Div([\n",
        "    html.H2(\"Data Selector\", style = {'height':'5%'}),\n",
        "    du.Upload(id='uploader', max_file_size=100, filetypes=['csv']  ),#  , upload_id='1234'),\n",
        "    html.Div(id='H1Aw' ),# , style={'display':'none'}),\n",
        "    html.Div(id='H2Aw', style={'display':'none'}),\n",
        "    #dcc.Upload(html.Button('Upload File', style={'aligned':'center'}), id='upload_file'),\n",
        "    #html.Hr(),\n",
        "\n",
        "    html.Div([\n",
        "      html.H6(\"Date Range of Data\", style={'float':'top'}),\n",
        "      dcc.DatePickerRange(\n",
        "        id='dateRange',\n",
        "        #display_format='MMM Do, YYYY',\n",
        "        #min_date_allowed=st_date,\n",
        "        #max_date_allowed=en_date,\n",
        "        initial_visible_month=st_date,\n",
        "        start_date=st_date,\n",
        "        end_date=en_date,\n",
        "        style={'margin':'0.5%', 'zoom':'auto', 'width':'99%', 'height':'33%', 'background-color': '#b0c4de'}\n",
        "      )\n",
        "    ], style = {'height':'14%', 'borderStyle': 'solid', 'borderColor': '#0F00', 'margin':'0.4%'}),\n",
        "    html.Hr(),\n",
        "\n",
        "    html.Div([\n",
        "      html.H6(\"Data Options\", style = {'height':'40%'}),\n",
        "      html.Div([\n",
        "        daq.BooleanSwitch(id='LiveDat', on=True, label='Live Data', style = {'float':'left', 'margin-right':'10px'}),\n",
        "        daq.BooleanSwitch(id='bckTest', on=True, label='Backtest Data', style = {'float':'left', 'margin-right':'10px'}),\n",
        "        daq.BooleanSwitch(id='fltrDat', on=True, label='Filter Data?', style = {'float':'left', 'margin-right':'10px'}),\n",
        "      ], style = {'height':'60%'}), \n",
        "    ], style = {'height':'14%', 'borderStyle': 'solid', 'borderColor': '#0F00', 'margin':'0.4%'}),\n",
        "    #html.H6('`'),\n",
        "    html.Hr(style={'float':'center'}),\n",
        "\n",
        "    \n",
        "    html.Div(id='H3Aw', style={'display':'none'}),\n",
        "    html.Div(id='H4Aw', style={'display':'none'}),\n",
        "    html.Div(id='H5Aw', style={'display':'none'}),\n",
        "    html.Div(id='H6Aw', style={'display':'none'}),\n",
        "    \n",
        "    #html.Div([\n",
        "    #  html.H3(\".\", id='H3Asw'),\n",
        "    #  html.Div(id='H3Aw', style={'display':'none'}),\n",
        "    #  html.H3(\"Data Filtering\", style = {'display':'inline','margin-top':'40%'}),\n",
        "    #  daq.BooleanSwitch(id='fltrDat', on=True, style={'display':'inline'}),\n",
        "    #  \n",
        "    #], style = {'height':'15%'}),\n",
        "    #html.Button(\"Update Data\", id='upt_butt', style = {'width':'90%'}),#, style = {'margin-left':'5%','margin-right':'5%', 'width':'90%'}),\n",
        "    html.Div([\n",
        "      html.H6(\"Plot Categories\"),\n",
        "      html.Button(\"Scatter Plot\", n_clicks =0, id='bt_sctr', className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      html.Button(\"3D Scatter Plot\", n_clicks =0, id='bt_st3d',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      html.Button(\"Aggregate Plot\", n_clicks =0, id='bt_aggr',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      html.Button(\"Parallel Plot\", n_clicks =0, id='bt_para',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      html.Button(\"Treemap Plot\", n_clicks =0, id='bt_tmap',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      html.Button(\"Generic Plots\", n_clicks =0, id='bt_gnrc',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      #html.Button(\"Plot_7\", n_clicks =0, id='button_7',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      #html.Button(\"Plot_8\", n_clicks =0, id='button_8',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'}),\n",
        "      #html.Button(\"Plot_9\", n_clicks =0, id='button_9',  className='plot_buttons', style = {'margin':'auto', 'width':'45%'})\n",
        "    ] , id='plot_buttons', style = {'height':'54%', 'borderStyle': 'solid', 'borderColor': '#0F00', 'margin':'0.4%'})\n",
        "  ],id = 'options_pane', style=style_left_pane),\n",
        "\n",
        "  html.Div([\n",
        "    html.Div([dcc.Graph(figure=fig, style={'height':'100%', 'width':'100%'}, id='graph')], id='Graph_pane', style=style_graph),\n",
        "    html.Hr(),\n",
        "\n",
        "    \n",
        "    html.Div([ html.H6(\"Plot Option Selectors\"),\n",
        "      scatter_UI, scatter3d_UI, aggr_UI, para_UI\n",
        "    ], id='plot_options_pane', style = style_plot_options)\n",
        "  ], id='graph_pane', style=style_right_pane)\n",
        "#], id='Main_Container', style={'display':'none'})])# style=style_main)])\n",
        "], id='Main_Container', style=style_main)])# style=style_main)])\n"
      ],
      "metadata": {
        "id": "Gh2dejW6HzzK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make Callbacks"
      ],
      "metadata": {
        "id": "rh_UrajR8TrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@du.callback(\n",
        "    output=Output('sdf', 'data'),\n",
        "    id=\"uploader\",\n",
        ")\n",
        "def callback_on_completion(filenames):\n",
        "    return json.dumps(filenames)\n",
        "\n",
        "#for du >0.7\n",
        "#@du.callback(\n",
        "#    output=Output('sdf', 'data'),\n",
        "#    id=\"uploader\",\n",
        "#)\n",
        "#def callback_on_completion(status: du.UploadStatus):\n",
        "#    return json.dumps(status.latest_file)\n",
        "\n",
        "@app.callback(\n",
        "    Output('upload_status', 'data'),\n",
        "    Output('upload_path', 'data'),\n",
        "    Output('H4Aw', 'children'),\n",
        "    Output('dateRange', 'start_date'),\n",
        "    Output('dateRange', 'end_date'),\n",
        "    Output('para_lifecycle', 'options'),\n",
        "    Output('para_setup', 'options'),\n",
        "    Output('para_setupFilters', 'options'),\n",
        "    Output('para_options', 'options'),\n",
        "    Output('para_entries', 'options'),\n",
        "    Output('para_exits', 'options'),\n",
        "    [Input('sdf', 'data')]\n",
        ")\n",
        "def check_upload( df_path ):#, json_DF ):\n",
        "  try:\n",
        "    df__path = json.loads(df_path)\n",
        "    red = pd.read_csv(df__path)\n",
        "    red, _, date_range, jckd,_ = preprocess_df(red)\n",
        "    df__path = df__path.replace('.csv', '_preprocced.csv')\n",
        "    red.to_csv(df__path, sep=',')\n",
        "    return json.dumps('Success!'), json.dumps(df__path), 'success! '+str(json.loads(df_path)), red['E_Dates'].iloc[0], red['E_Dates'].iloc[-1], jckd['lifecycle'], jckd['setup'], jckd['setupFilters'], jckd['options'], jckd['entries'], jckd['exits']\n",
        "  except Exception as e1:\n",
        "    try:\n",
        "      df__path = json.loads(df_path)[0]\n",
        "      red = pd.read_csv(df__path)\n",
        "      red, _, date_range, jckd,_ = preprocess_df(red)\n",
        "      df__path = df__path.replace('.csv', '_preprocced.csv')\n",
        "      red.to_csv(df__path, sep=',')\n",
        "      return json.dumps('Success!'), json.dumps(df__path), 'success! '+str(json.loads(df_path)), red['E_Dates'].iloc[0], red['E_Dates'].iloc[-1], jckd['lifecycle'], jckd['setup'], jckd['setupFilters'], jckd['options'], jckd['entries'], jckd['exits']\n",
        "    except Exception as e:\n",
        "      return json.dumps('Failure'), json.dumps(''), 'Failure! '+'E: '+str(e)+'F: '+str(df_path), df_full['E_Dates'].iloc[0], df_full['E_Dates'].iloc[-1], json_column_keys_dict['lifecycle'], json_column_keys_dict['setup'], json_column_keys_dict['setupFilters'], json_column_keys_dict['options'], json_column_keys_dict['entries'], json_column_keys_dict['exits']\n",
        "\n",
        "\"\"\"\n",
        "@app.callback(\n",
        "    Output(component_id='mem_df', component_property='data'),\n",
        "    Output('H5Aw', 'children'),\n",
        "    [Input('upload_status', 'data'),\n",
        "    Input('upload_path', 'data')]\n",
        ")\n",
        "def make_df( status, path ):\n",
        "  status, path = json.loads(status), json.loads(path)\n",
        "  if status=='Success!':\n",
        "    try:\n",
        "      red = pd.read_csv(path)\n",
        "      red, _, _, _, _ = preprocess_df(red)\n",
        "      return red.to_json(date_format='iso', orient='split'), 'Success!'+'F: '+str(path)\n",
        "    except Exception as e:\n",
        "      return df_full.to_json(date_format='iso', orient='split'), 'Failure! '+'F: '+str(path)+'E: '+str(e)\n",
        "  else:\n",
        "    return df_full.to_json(date_format='iso', orient='split'), 'Default'\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "@app.callback(#[Output('output-data-upload', 'children'),\n",
        "              Output('sdf', 'data'),\n",
        "              Input('upload_file', 'contents'),\n",
        "              State('upload_file', 'filename'),\n",
        "              State('upload_file', 'last_modified'))\n",
        "def update_output(list_of_contents, list_of_names, list_of_dates):\n",
        "    if list_of_contents is not None:\n",
        "        children = [\n",
        "            parse_contents(c, n, d) for c, n, d in\n",
        "            zip(list_of_contents, list_of_names, list_of_dates)]\n",
        "        #return children, \n",
        "        return [parse_contents_df(c, n, d) for c, n, d in zip(list_of_contents, list_of_names, list_of_dates)][0].to_json(date_format='iso', orient='split')\n",
        "\"\"\"\n",
        "\n",
        "@app.callback(\n",
        "    Output('graph_sel', 'data'),\n",
        "    Output(component_id='sui_div', component_property='style'),\n",
        "    Output(component_id='3dui_div', component_property='style'),\n",
        "    Output(component_id='aggr_div', component_property='style'),\n",
        "    Output(component_id='para_div', component_property='style'),\n",
        "    Output(component_id='plot_options_pane', component_property='style'),\n",
        "    Output(component_id='graph', component_property='style'),\n",
        "    [Input('bt_sctr', 'n_clicks'),\n",
        "    Input('bt_st3d', 'n_clicks'),\n",
        "    Input('bt_aggr', 'n_clicks'),\n",
        "    Input('bt_para', 'n_clicks'),\n",
        "    Input('bt_tmap', 'n_clicks'),\n",
        "    Input('bt_gnrc', 'n_clicks')]\n",
        ")\n",
        "def select_graph( btn1, btn2, btn3, btn4, btn5, btn6):\n",
        "  changed_id = [p['prop_id'] for p in callback_context.triggered][0]\n",
        "  if 'bt_sctr' in changed_id:\n",
        "    cur_graph = 'sctr'\n",
        "    return json.dumps(cur_graph), {'display':'block'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'block'}, {'height': '100%'}\n",
        "  elif 'bt_st3d' in changed_id:\n",
        "    cur_graph = 'sctr3d'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'block'}, {'display':'none'}, {'display':'none'}, {'display':'block'}, {'height': '100%'}\n",
        "  elif 'bt_aggr' in changed_id:\n",
        "    cur_graph = 'aggr'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'none'}, {'display':'block'}, {'display':'none'}, {'display':'block'}, {'height': '100%'}\n",
        "  elif 'bt_para' in changed_id:\n",
        "    cur_graph = 'para'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'block'}, {'display':'block'}, {'height': '100%'}\n",
        "  elif 'bt_tmap' in changed_id:\n",
        "    cur_graph = 'tmap'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'height': '150%'}\n",
        "  elif 'bt_gnrc' in changed_id:\n",
        "    cur_graph = 'gnrc'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'height': '150%'},\n",
        "  else:\n",
        "    cur_graph = 'None'\n",
        "    return json.dumps(cur_graph), {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'display':'none'}, {'height': '100%'}\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output('sui_state', 'data'),\n",
        "    [Input('sui_xaxis', 'value'),\n",
        "    Input('sui_yaxis', 'value'),\n",
        "    Input('sui_size_col', 'value'),\n",
        "    Input('sui_color_col', 'value'),\n",
        "    Input('sui_symbol_col', 'value'),\n",
        "    Input('sui_facet_col', 'value'),\n",
        "    Input('sui_hover_col', 'value')]\n",
        ")\n",
        "def sui_state_update( x,y,s,c,sym,f,h ):\n",
        "  return json.dumps([x,y,s,c,sym,f,h])\n",
        "\n",
        "@app.callback(\n",
        "    Output('sui3d_state', 'data'),\n",
        "    [Input('sui3d_xaxis', 'value'),\n",
        "    Input('sui3d_yaxis', 'value'),\n",
        "    Input('sui3d_zaxis', 'value'),\n",
        "    Input('sui3d_size_col', 'value'),\n",
        "    Input('sui3d_color_col', 'value'),\n",
        "    Input('sui3d_symbol_col', 'value'),\n",
        "    Input('sui3d_hover_col', 'value')]\n",
        ")\n",
        "def sui3d_state_update( x,y,z,s,c,sym,h ):\n",
        "  return json.dumps([x,y,z,s,c,sym,h])\n",
        "\n",
        "@app.callback(\n",
        "    Output('aggr_state', 'data'),\n",
        "    [Input('aggr_xaxis', 'value'),\n",
        "    Input('aggr_yaxis', 'value'),\n",
        "    Input('aggr_color_col', 'value')]\n",
        ")\n",
        "def aggr_state_update( x,y,c ):\n",
        "  return json.dumps([x,y,c])\n",
        "\n",
        "@app.callback(\n",
        "    Output('para_state', 'data'),\n",
        "    #Output('H3Aw', 'children'),\n",
        "    [Input('para_result_cats', 'value'),\n",
        "    Input('para_lifecycle', 'value'),\n",
        "    Input('para_setup', 'value'),\n",
        "    Input('para_setupFilters', 'value'),\n",
        "    Input('para_options', 'value'),\n",
        "    Input('para_entries', 'value'),\n",
        "    Input('para_exits', 'value')]\n",
        ")\n",
        "def para_state_update(p1, p2, p3, p4, p5, p6 ,p7):\n",
        "  #return json.dumps(list(p1)+list(p2)+list(p3)+list(p4)+list(p5)+list(p6)+list(p7))\n",
        "  comb = []\n",
        "  comb = comb+p1 if p1 else comb\n",
        "  comb = comb+p2 if p2 else comb\n",
        "  comb = comb+p3 if p3 else comb\n",
        "  comb = comb+p4 if p4 else comb\n",
        "  comb = comb+p5 if p5 else comb\n",
        "  comb = comb+p6 if p6 else comb\n",
        "  comb = comb+p7 if p7 else comb\n",
        "  comb = comb if comb else result_cats\n",
        "  return json.dumps(comb)\n",
        "\n",
        "@app.callback(\n",
        "    Output('options_state', 'data'),\n",
        "    [Input('dateRange', 'start_date'),\n",
        "    Input('dateRange', 'end_date'),\n",
        "    Input('LiveDat', 'on'),\n",
        "    Input('bckTest', 'on'),\n",
        "    Input('fltrDat', 'on')]\n",
        ")\n",
        "def aggr_state_update( s,e, l,b,f ):\n",
        "  ret = [s, e]\n",
        "  ret = ret+[1] if l else ret+[0]\n",
        "  ret = ret+[1] if b else ret+[0]\n",
        "  ret = ret+[1] if f else ret+[0]\n",
        "  return json.dumps(ret)\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(component_id='graph', component_property='figure'),\n",
        "    Output('H3Aw', 'children'),\n",
        "    Output('H1Aw', 'children'),\n",
        "    [Input('graph_sel', 'data'),\n",
        "    Input('sui_state', 'data'),\n",
        "    Input('sui3d_state', 'data'),\n",
        "    Input('aggr_state', 'data'),\n",
        "    Input('para_state', 'data'),\n",
        "    Input('options_state', 'data'),\n",
        "    Input('sdf', 'data'),\n",
        "    Input('upload_path', 'data'),\n",
        "    Input('upload_status', 'data'),\n",
        "    #Input(component_id='wdf', component_property='data'),\n",
        "     ]\n",
        ")\n",
        "def make_graph( graph_sel, sui_state, sui3d_state, aggr_state, para_state, options_state, json_DF, upload_path, upl_status):#, json_DF ):\n",
        "  out_string = 'Upload Status: '\n",
        "  try:\n",
        "    ot_opts = json.loads(options_state)\n",
        "    upl_status = json.loads(upl_status)\n",
        "    if upl_status=='Success!':\n",
        "      out_string = out_string + 'Updated'\n",
        "      wdf = make_wdf(pd.read_csv(json.loads(upload_path)), ot_opts)\n",
        "      #wdf = pd.read_csv(json.loads(upload_path))\n",
        "      upl_status = upl_status+'_loaded_'+json.loads(upload_path)\n",
        "    else:\n",
        "      out_string = out_string + 'Default/Pending'\n",
        "      wdf = make_wdf(df_full, ot_opts)\n",
        "      upl_status = upl_status+'_NL'\n",
        "    #wdf = pd.read_json(json_DF, orient='split')\n",
        "    \n",
        "    graph_sel = json.loads(graph_sel)\n",
        "    if graph_sel == 'sctr':\n",
        "      options = json.loads(sui_state)\n",
        "      fig = make_scatter(wdf, options[0], options[1], options[2], options[3], options[4], options[5], options[6])\n",
        "      return fig, str(upl_status), out_string\n",
        "    elif graph_sel == 'sctr3d':\n",
        "      options = json.loads(sui3d_state)\n",
        "      return make_3dscatter(wdf, options[0], options[1], options[2], options[3], options[4], options[5], options[6]), str(options), out_string\n",
        "    elif graph_sel == 'aggr':\n",
        "      options = json.loads(aggr_state)\n",
        "      return make_line(wdf, options[0], options[1], options[2]), str(options), out_string\n",
        "    elif graph_sel == 'para':\n",
        "      options = json.loads(para_state)\n",
        "      return make_parallel(wdf, options), str(options), out_string\n",
        "    elif graph_sel == 'tmap':\n",
        "      options = 'tmap'\n",
        "      return make_treemap_anim(wdf), str(options), out_string\n",
        "    elif graph_sel == 'gnrc':\n",
        "      options = json.loads(sui_state)\n",
        "      fig = go.Figure()\n",
        "      fig.add_trace(go.Scatter(x=[5,1,3,9], y=[5,-1, 7, -4]))\n",
        "      fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "      return fig, 'Generic selected', out_string\n",
        "    else:\n",
        "      fig = go.Figure()\n",
        "      fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "      return fig, 'None selected', out_string\n",
        "  except Exception as e:\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=[0, 12, 12, 0, 0], y=[0, 0, 10, 10, 0]))\n",
        "    fig.add_annotation(x=6, y=6, text=\"Couldn't generate plot due to following error:\", showarrow=False, yshift=10)\n",
        "    fig.add_annotation(x=6, y=5, text=\"{}\".format(e), showarrow=False, yshift=10)\n",
        "    fig.update_layout(paper_bgcolor=\"LightSteelBlue\")\n",
        "    return fig, 'Erreur', out_string"
      ],
      "metadata": {
        "id": "6lYkNQ8x8LM_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Dash"
      ],
      "metadata": {
        "id": "8vH0i3gQ8PU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  app.run_server(mode='external', debug=True)"
      ],
      "metadata": {
        "id": "hPbFhVO28N-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b16c936c-4840-475c-fbb8-8fd8de6a15a2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dash app running on:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8050, \"/\", \"http://127.0.0.1:8050/\", window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TAVF_Dash.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}